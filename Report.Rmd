---
title: "Building Predictive Text Models for Twitter, News, and Blogs Corpora"
author: Mykola Steshenko
output: html_document
---

## Synopsis

The obsession of smartphones and other mobile devices has significantly affected our life, and we spend much time typing different kinds of texts in them. The typing still remains very unconvinient due to the size of devices, their virtual keyboards, sensor monitors and other factors. Smart keyboards can improve effeciency of this proccess, reducing the amount of typing. The main goal of this project is to develope a predictive text model for such keyboard, which will be able to predict the most probable next words for twitter, news, and blogs texts. Obtained model will be implemented into a demonstrational Web application.

## Data

```{r status_check, echo=FALSE}
# This code checks if the current working directory contains any of the
# output files produced by this script. If it successfully finds any, it will start 
# analysis from the corresponding checkpoint

status <- c(file.exists("Coursera-SwiftKey.zip"),
            all(file.exists("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")),
            all(file.exists("gram1.twitter.counts", "gram2.twitter.counts", 
                            "gram3.twitter.counts", "gram4.twitter.counts", 
                            "gram1.blogs.counts", "gram2.blogs.counts", 
                            "gram3.blogs.counts", "gram4.blogs.counts", 
                            "gram1.news.counts", "gram2.news.counts", 
                            "gram3.news.counts", "gram4.news.counts")),
            all(file.exists("twitter.model", "blogs.model")))#, "news.model")))

```

### Data source

The data for this project is from [HC Corpora](www.corpora.heliohost.org). The particular dataset, that was used for analysis, was provided by [Coursera](https://www.coursera.org/) and can be downloaded here:

[Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

Zip archive contains files in format LOCALE.blogs.txt where LOCALE is the each of the four locales en\_US, de\_DE, ru\_RU and fi\_FI. We analysed only English corpora in this project, which consists of three files "en\_US.twitter.txt", "en\_US.blogs.txt", and "en\_US.news.txt". As it mentioned in the names of the files, they contain raw textual data from twitter, blogs and news respectively. The files also might contain some foreign text and words of offensive and profane meaning. Here is basic information about size and counts of lines, words, and symbols for each file:

```{r info, echo=FALSE, echo=FALSE}
info <- data.frame(row.names=c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"), 
                   line_counts=c(899288, 1010242, 2360148), 
                   word_counts=c(37334131, 34372530, 30373583), 
                   sizeMb=c(210160014 / (1024 ** 2), 
                            205811889 / (1024 ** 2), 
                            167105338 / (1024 ** 2)))
info
```

Additional description for this dataset is available in HC Corpora [ReadMe](http://www.corpora.heliohost.org/aboutcorpus.html).

### Data processing

```{r download, echo=FALSE, echo=FALSE}
if (!any(status)) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip", mode="wb")
   unzip("Coursera-SwiftKey.zip", junkpaths=TRUE, 
         files=c("final/en_US/en_US.twitter.txt", 
                 "final/en_US/en_US.news.txt", 
                 "final/en_US/en_US.blogs.txt"))
}
```

The data in each file was separated into two parts. The first part, which consists of roughly 80% of data, was used as training set for n-gram counts generation and for building the predictive model. The rest of the data was used as a test set for the model performance evaluation. Here is data distribution for all three files:

1. en_US.twitter.txt - 2,000,000 lines in training set, 360,148 lines in test set
2. en_US.blogs.txt - 750,000 lines in training set, 149,288 lines in test set
3. en_US.news.txt - 800,000 lines in training set, 210,238 lines in test set

#### Data Cleaning

The raw data contains upper-case letters, punctuation marks, textual emotion expressions (different kinds of smiles), digits, e-mails, hash-tags, offensive and profane words, duplicated symbols, and some other special symbols that should be handled before further processing. It is expected that data from twitter have much more issues like these. Due to this fact, twitter predictive model also might have worse performance than other models, while data from blogs and news was written in much more clear way. Libraries "tm" and "stringi" were used for data cleaning. At the first step of cleaning process all documents in corpus were transformed to the lower case. After that symbols ">" and "<" were removed, because they might be useful later as special tags. At the next step all digits and combinations of digits with digits separated by one of special symbols (":,.-") were replaced with tag "\<digit\>", because the exact value of them can't be useful in prediction model. Also all e-mails were replaced with tag "\<e-mail\>" for the same reasons. After that some smiles like ":-P" that might produce single-letter tokens after the special symbols deletion were removed.

Another important procedure in a data cleaning process is handling of punctuation marks and other special symbols that might be meaningful. For instance, we can't just remove symbols like "-" and different kinds of apostrophes, because they play important role in some combined words like "I'm", "don't", "e-mail", etc. Also we can't remove all meaningful punctuation marks (",.?!"), which might cause chaos in corpus and produce n-grams that normally do not appear in sentence. Before handling punctuations, symbol ";" was replaced with "," due to the close meaning. We decided to left punctuation marks ",.?!" and treat them like separate words in our prediction model, because in this case they can be used also for the punctuation prediction, while they can be easily filtered at the final stage of the project. After handling these special symbols all other symbols "\)\:\#\%\$\^\\\~\}\&\+\=\@\/\"\_\]\+" were removed.

We did not remove hash-tags but only hash-tag symbols, because they usually are meaningful. Also we decided not to remove stop words from corpus. For more convenience all the cleaning procedures described above were placed into one function, that was used in data parser.

```{r cleaning, echo=FALSE}
cleanCorpus <- function(corpus) {
    # Makes some cleaning and symbol handling procedures for tm corpus of textdata
    library(tm)
    library(stringi)
    # Case correction
    corpus  <- tm_map(corpus, function(x) stri_trans_tolower(x[[1]]))
    # Remove tags
    corpus  <- tm_map(corpus, function (x) gsub("[<>]", "", x[[1]]))
    # digits replacement
    corpus  <- tm_map(corpus, function (x) gsub("[[:digit:]]+[:.,-][[:digit:]]+", " <digit> ", x[[1]]))
    corpus  <- tm_map(corpus, function (x) gsub("[[:digit:]]+", " <digit> ", x[[1]]))
    # email replacement
    corpus  <- tm_map(corpus, function (x) gsub("[[:alnum:].-]+@[[:alnum:].-]+", " <e-mail> ", x[[1]]))
    # remove some smiles that might left standalone alphabetic symbols after deletion
    for (smile in c("x-p", "o.o", "[=:;][boqje#pds]")) {
        corpus  <- tm_map(corpus, function (x) gsub(smile, "", x[[1]]))
    }
    # replace ?!?!?!?! and ;
    corpus  <- tm_map(corpus, function (x) gsub("\\?+\\!+|\\!+\\?+", "?", x[[1]]))
    corpus  <- tm_map(corpus, function (x) gsub(";+", ",", x[[1]]))
    # remove duplicates of important punctuation and add spacing
    for (punct in c("!", "?", ".", ",")) {
        pattern <- paste0("\\", punct, "+")
        replacement <- paste0(" ", punct, " ")
        corpus  <- tm_map(corpus, function (x) gsub(pattern, replacement, x[[1]]))
    }
    # remove other punctuation
    corpus  <- tm_map(corpus, function (x) gsub('[])(:#%$^\\~{}[&+=@/"_]+', "", x[[1]]))
    return(corpus)
}
```


#### Profanity Filtering

Another important task in data cleaning is the removing of offensive and profane words, because we usually do not want to predict them. Just deleting these words might cause some problems with text consistency, so we replaced them with special tag "\<badword\>". The universal regular expression for the most common profane words was created using freely available [bad-words dictionary](http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/). 

```{r badwords, echo=FALSE}
badWordHandle <- function(corpus) {
    # Change most frequent badwords to <badword> tag
    reg <- " screw | wank[a-z]*|b[o0][o0]bs|l3i[a-z]ch|knob|or[ia]f[ai][cs]|[a-z]damn |d[iy]ck|q[uw]e[ei]r| arse|[ck]awk| gay|retard|s[kc]anc*k|t[ie]+t[sz]|slut|puss[ye]|vul+va|vag*[i1]*j*i*na|pola[ck]|phu[ck]|pe[ie]*n[a1iu]+s|pac*k[iy]| anal | anus |butt[wh-]|clit|crap|pu[tl][eao] |jar*c*k[\\s-]*off|blow\\s*job|ji[zs]+|nazi|w*h[o0]+a*r|a[s]*h[o0]le|sh[i1!y]t|mas+t[eu]*rbai*t|n[i1]+g+[eu]*r| a[sz][sz]|bas+t[ae]rd|b[!i1][a7]*tch|c[o0]ck| cum |[ck]u*nt|dild[a-z]*|f\\s*u\\s*c*\\s*k|fai*g"
    corpus  <- tm_map(corpus, function (x) gsub(reg, " <badword> ", x[[1]]))
    return(corpus)
}
```


#### White-space trim and empty string deletion

At the next step of data processing the white-space correction was performed. Firstly, all white-space characters at the beginning and at the end of each document were removed, as well as duplicates of them. Secondly, empty strings were deleted from corpus. Finally, special tags "\<s\>",that means the start of a document, were placed at the beginning of each document in a corpus. We did this for the reason to be able to predict words at the beginning of the text, when there is no words to predict from. All these white-space processing procedures were placed into one function that was used in parser.

```{r whitespace, echo=FALSE}
whiteSpaceCorrection <- function(corpus) {
    # Provides end/begin whitespace trimming an removing of whitespace duplicates
    corpus  <- tm_map(corpus, function (x) gsub("^\\s+|\\s+$", "", x[[1]]))
    # remove 1+ spaces
    corpus  <- tm_map(corpus, function (x) gsub("\\s+", " ", x[[1]]))
    # empty string deletion
    item_length <- tm_map(corpus, function (x) nchar(x[[1]]))
    item_length <- unlist(item_length)
    idx <- item_length == 0
    corpus <- corpus[!idx]
    # starting each document with special tag "<s> "
    corpus  <- tm_map(corpus, function (x) paste("<s>", x[[1]]))
    return(corpus)
}
```


### n-gram Counts

After we obtained a clean corpus of documents, the next important step in the process of data model creation is a word tokenization and n-gram counting. N-grams might be counted by making n-gram matrices using standard function from "tm" package. But there are some problems with this approach. Firstly, data matrices produced with it are very sparse and mostly consists of zeroes. Secondly, due to the previous fact, it is quite slow process. Thirdly, we have reasonable amount of data, and there are millions of n-grams there. It is not possible to store such a big matrix in PC memory. One way to handle this problem is to do not use matrices for counts storing. For a model creation we need counts only for n-grams that occurred in the data and there is no need in all zeroes in sparse matrix. The better way to store counts is a dictionary-like structures, for instance a list or named vector.

Another problem is in the size of the data. Even very simple operations like indexing slow down significantly when the size of an n-gram storage object increases. To handle this problem we decided to use "map-reduce like" approach. The corpus of data can be mapped to the small chunks very easy, so even without parallel computing this might improve performance considerably, at the same time avoiding memory restrictions. During the analysis each corpus was mapped to the small chunks and 1, 2, 3 and 4-grams were counted. At the reduce stage all these counts were merged into the total 1, 2, 3 and 4-grams counts.

#### Map Phase

N-grams was counted with the function that takes a small chunk of data as an input and writes 1, 2, 3 and 4-grams counts to global storage objects.

```{r ngram_counts, echo=FALSE}
n_gramCount <- function (data) {
    # writes counts of all possible 1, 2, 3 and 4 grams from corpus to 
    # global ngram storage objects "gram1", "gram2", "gram3", "gram4"
#     count <<- count + 1
#     if (count %% 100 == 0) {      # performance testing
#         print(paste(count, "documents evaluated", date()))
#     }
    onegrams <- strsplit(data, split=" ")[[1]]
    for (index in 1:length(onegrams)) {
        # 1-gram count
        word1 <- onegrams[index]
        value <- gram1[[word1]]
        if (!is.null(value)) {
            gram1[[word1]] <<- value + 1
        } else {
            gram1[[word1]] <<- 1
        }
        # 2-gram count
        word2 <- onegrams[index + 1]
        if (!is.na(word2)) {
            twogram <- paste(word1, word2)
            value <- gram2[[twogram]]
            if (!is.null(value)) {
                gram2[[twogram]] <<- value + 1
            } else {
                gram2[[twogram]] <<- 1
            }
        }
        # 3-gram count
        word3 <- onegrams[index + 2]
        if (!is.na(word3)) {
            threegram <- paste(word1, word2, word3)
            value <- gram3[[threegram]]
            if (!is.null(value)) {
                gram3[[threegram]] <<- value + 1
            } else {
                gram3[[threegram]] <<- 1
            }
        }
        # 4-gram count
        word4 <- onegrams[index + 3]
        if (!is.na(word4)) {
            fourgram <- paste(word1, word2, word3, word4)
            value <- gram4[[fourgram]]
            if (!is.null(value)) {
                gram4[[fourgram]] <<- value + 1
            } else {
                gram4[[fourgram]] <<- 1
            }
        }
    }
}
```

The higher level parser function that takes a name of file, the size of a chunk, start and end points as an input were developed. It uses n-gram counting function to count n-grams and store counts to a hard disk drive into a single file for each chunk.

```{r parser, echo=FALSE}
parseData <- function(filename, chunksize=1000, start_from=1, end_at=4000) {
    # This function takes file with raw data by small chunks and
    # provides data cleaning, counting n-grams and storing them into files
    library(tm)
    con <- file(filename, open="rb")
    num <- 0
    ngram_list <- grep("ngram", dir(), value=TRUE)
    if (length(ngram_list) != 0) {
        last_file_done <- max(as.integer(unlist(sapply(strsplit(ngram_list, split="_"), function (x) {x[2]}))))
    } else {
        last_file_done <- 0
    }
#     print(paste("Started at", date()))
    while (length(data <- readLines(con, chunksize, encoding="utf-8", skipNul=TRUE)) > 0 & num < end_at) {
        if (num + 1 > last_file_done & num + 1 >= start_from) {
            corpus <- VCorpus(VectorSource(data), readerControl=list(reader=readPlain, language="en"))
            corpus <- cleanCorpus(corpus)
            corpus <- badWordHandle(corpus)
            corpus <- whiteSpaceCorrection(corpus)
            gram1 <<- list()
            gram2 <<- list()
            gram3 <<- list()
            gram4 <<- list()
            res <- sapply(corpus, function (x) n_gramCount(x[[1]]))
            gram1 <<- unlist(gram1)
            gram2 <<- unlist(gram2)
            gram3 <<- unlist(gram3)
            gram4 <<- unlist(gram4)
            rm(res)
            save(gram1, gram2, gram3, gram4, file=paste0("ngram_", num+1))
#             print(paste("Elements from", (num)*chunksize, "to", (num+1)*chunksize, "were evaluated", date()))  # performance testing
        }
        num <- num + 1
    }
    close(con)
}
```

We decided to count n-grams and to create the prediction model for each file separately. There are some reasons for that, and the main is that prediction models are good only in context of the data. While we still can use models from blog corpus to predict words in news text, we definitely shouldn't do this for twitter data. Data in them is very different and accuracy of this predictive model will be not very high. So we decided to create three different models for each context (news, blogs and twitter).

#### Reduce Phase

In the reduce phase we merged all the n-grams counts from the all files created in the map phase. Three different functions was created to perform this. The first one splits files with 1, 2, 3 and 4-gram counts into four separate files. Second takes two files with n-gram counts of the same rank and merge them together. And the third is a higher level function that goes over all files and merge them for each given n.

```{r reducers, echo=FALSE}
separateNgramFiles <- function(filenames_list) {
    # Separate composit ngram files to files that contains only one type of ngrams
    count <- 0
    for (name in filenames_list) {
        load(name)
        save(gram1, file=paste0(name, "_1"))
        save(gram2, file=paste0(name, "_2"))
        save(gram3, file=paste0(name, "_3"))
        save(gram4, file=paste0(name, "_4"))
        count <- count + 1
#         if (count %% 10 == 0) {  # performance testing
#             print(paste(count, "files were processed"))
#         }
        file.remove(name)
    }
}


mergeNgramFiles <- function(a, b) {
    # merge two n-gram files
    n <- substr(a, nchar(a), nchar(a))
    load(a)
    if (n == 1) {
        first <- gram1
        rm(gram1)
    } else if (n == 2) {
        first <- gram2
        rm(gram2)
    } else if (n == 3) {
        first <- gram3
        rm(gram3)
    } else {
        first <- gram4
        rm(gram4)
    }
    load(b)
    if (n == 1) {
        second <- gram1
        rm(gram1)
    } else if (n == 2) {
        second <- gram2
        rm(gram2)
    } else if (n == 3) {
        second <- gram3
        rm(gram3)
    } else {
        second <- gram4
        rm(gram4)
    }
    unique_names <- unique(c(names(first), names(second)))
    empty <- integer(length(unique_names))
    names(empty) <- unique_names
    ad <- data.frame(empty, empty)
    ad[names(first), 1] <- first
    ad[names(second), 2] <- second
    result <- rowSums(ad)
    file.remove(a)
    file.remove(b)
    if (n == 1) {
        gram1 <- result
        rm(result)
        save(gram1, file=a)
    } else if (n == 2) {
        gram2 <- result
        rm(result)
        save(gram2, file=a)
    } else if (n == 3) {
        gram3 <- result
        rm(result)
        save(gram3, file=a)
    } else {
        gram4 <- result
        rm(result)
        save(gram4, file=a)
    }
    
}


mergeAll <- function(n) {
    # Merge all n-gram files for the given n
    files <- grep(paste0("ngram_[[:digit:]]+_", n), dir(), value=TRUE)
    file_info <- file.info(dir())
    too_big <- file_info$size
    names(too_big) <- rownames(file_info)
    too_big <- names(too_big[too_big > 6e7])
    files <- setdiff(files, too_big)
    while (length(files) > 1) {
        files <- grep(paste0("ngram_[[:digit:]]+_", n), dir(), value=TRUE)
        file_info <- file.info(dir())
        too_big <- file_info$size
        names(too_big) <- rownames(file_info)
        too_big <- names(too_big[too_big > 6e7])
        files <- setdiff(files, too_big)
        if (length(files) > 1) {
            indexes <- seq(from=1, to=length(files)-1, by=2)
            for (index in indexes) {
                mergeNgramFiles(files[index], files[index+1])
            }
        }
    }
}
```

#### n-gram Counts Optimization

The final aim of this project is a Web application that might be used on mobile gadgets for the next word prediction and this fact produces some restrictions on our predictive model. It should be reasonably small, does not require a lot of resources, and to have a high performance, because it should work in real-time manner. One possible way to make optimization without affecting the accuracy of the model is to delete some very rare n-grams.

This might also cause some other positive effects. We did not even tried to filter words in corpora to find some misspelled words, words from other languages, and random sets of symbols like "dadasfga sfghasgdf" during the cleaning process. The intuition was that words like these should be very rare in the dataset and after n-gram counting we can just remove n-grams with the lowest counts. We found that the percentage of the most frequent words that cover most n-gram counts differ for different n (presented results are for a subset of original data):

+ roughly top 0.6% 1-grams contain 50% of counts and top 30% 1-grams contain 90% of counts
+ roughly top 16% 2-grams contain 50% of counts and top 83% 2-grams contain 90% of counts
+ roughly top 45% 3-grams contain 50% of counts and top 89% contain 90% of counts

For 3-grams counts are distributed more uniformly than for 2 and 1-grams. This was expected because there are more possible combinations for n-grams with bigger n, and we do not have enough data to make a good coverage for all 3 and 4-grams. That is why removing n-grams with the count 1 might be not the best choice in the case of 4-grams. Despite this fact we decided to remove all n-grams with counts 1 for every n to improve performance.

```{r one_counters, echo=FALSE}
reduceOneCounters <- function(ngram) {
    # reduce n-grams deleting one-counters
    ngram <- sort(unlist(ngram), decreasing=TRUE)
    idx <- ngram == 1
    ngram <- ngram[!idx]
    return(ngram)
}
```

```{r all_counts, echo=FALSE}
if (!any(status[3:length(status)])) {
    parseData("en_US.twitter.txt", chunksize=500, start_from=1, end_at=4000)
    separateNgramFiles(grep("ngram", dir(), value=TRUE))
    mergeAll(1)
    mergeAll(2)
    mergeAll(3)
    mergeAll(4)
    load("ngram_1_1")
    gram1 <- reduceOneCounters(gram1)
    save(gram1, file="gram1.twitter.counts")
    rm(gram1)
    load("ngram_1_2")
    gram2 <- reduceOneCounters(gram2)
    save(gram2, file="gram2.twitter.counts")
    rm(gram2)
    load("ngram_1_3")
    gram3 <- reduceOneCounters(gram3)
    save(gram3, file="gram3.twitter.counts")
    rm(gram3)
    load("ngram_1_4")
    gram4 <- reduceOneCounters(gram4)
    save(gram4, file="gram4.twitter.counts")
    rm(gram4)
    
    
    parseData("en_US.blogs.txt", chunksize=50, start_from=1, end_at=15000)
    separateNgramFiles(grep("ngram", dir(), value=TRUE))
    mergeAll(1)
    mergeAll(2)
    mergeAll(3)
    mergeAll(4)
    load("ngram_1_1")
    gram1 <- reduceOneCounters(gram1)
    save(gram1, file="gram1.blogs.counts")
    rm(gram1)
    load("ngram_1_2")
    gram2 <- reduceOneCounters(gram2)
    save(gram2, file="gram2.blogs.counts")
    rm(gram2)
    load("ngram_1_3")
    gram3 <- reduceOneCounters(gram3)
    save(gram3, file="gram3.blogs.counts")
    rm(gram3)
    load("ngram_1_4")
    gram4 <- reduceOneCounters(gram4)
    save(gram4, file="gram4.blogs.counts")
    rm(gram4)
    
    
    parseData("en_US.news.txt", chunksize=100, start_from=1, end_at=8000)
    separateNgramFiles(grep("ngram", dir(), value=TRUE))
    mergeAll(1)
    mergeAll(2)
    mergeAll(3)
    mergeAll(4)
    load("ngram_1_1")
    gram1 <- reduceOneCounters(gram1)
    save(gram1, file="gram1.news.counts")
    rm(gram1)
    load("ngram_1_2")
    gram2 <- reduceOneCounters(gram2)
    save(gram2, file="gram2.news.counts")
    rm(gram2)
    load("ngram_1_3")
    gram3 <- reduceOneCounters(gram3)
    save(gram3, file="gram3.news.counts")
    rm(gram3)
    load("ngram_1_4")
    gram4 <- reduceOneCounters(gram4)
    save(gram4, file="gram4.news.counts")
    rm(gram4)
}
```

## Exploratory Data Analysis

### Blog Corpus

As the result of data cleaning and processing we obtained the n-gram counts for blog, twitter and news corpora. Here you can find information about the size of the different n-grams for blog corpus:

```{r load_blogs, cache=TRUE, echo=FALSE}
library(RColorBrewer)
load("gram1.blogs.counts")
load("gram2.blogs.counts")
load("gram3.blogs.counts")
load("gram4.blogs.counts")
size <- unlist(list(one.gram=length(gram1), two.gram=length(gram2),
                  three.gram=length(gram3), four.gram=length(gram4)))
res <- data.frame(n.gram.size=size, row.names=names(size))
res
```

The largest n-gram count object is for 3-grams (as well as for the other corpora in our dataset). This is because we removed n-grams that happened only once in the corpus. 4-grams contain more such "rare" n-grams that other, because there are more possible 4-grams than n-grams with smaller n. We do not have enough of data to cover all common 4-grams, so there are a lot of 4-grams with count 1, even though some of them are not really rare.

We analysed frequencies of different words in data. The 30 most frequent words in blog corpus are depicted on this graph:

```{r blogs_frequent_words, cache=TRUE, fig.height=4, fig.width=6, echo=FALSE}
par(mfrow=c(1, 1))
idx <- names(gram1) %in% c(".", "!", "?", ",", "<s>")
gr1 <- gram1[!idx]
col <- brewer.pal(9, "Greens")
col <- colorRampPalette(rev(col))
par(mar=c(6, 5, 3, 0))
barplot(head(gr1, 30), xlab="", ylab="Counts", col=col(30), las=2, ylim=c(0, 2000000),
        main="Most frequent words in blog corpus", cex.axis=0.6)
```

As we can see, the most common words are the "stop-words". It was expected, because we consciously did not remove them from the corpus during the cleaning process. We did not do that because they might be useful for the word prediction Web application, which is the final goal of this project. We might ignore them for some other tasks but predicting the next word in the sentence require them in the prediction model.

Apart the "stop-words", the blog corpus contains many digits.

Here you can see the bar-charts of the most frequent n-grams in the blog corpus:

```{r blogs_grams,  cache=TRUE, fig.height=5, fig.width=7, echo=FALSE}
par(mfrow=c(2, 2))
par(mar=c(5, 5, 2, 0))
col <- brewer.pal(9, "Greens")
col <- colorRampPalette(rev(col))
barplot(head(gram1, 20), xlab="", col=col(20), las=2, ylim=c(0, 2000000),
        main="Counts of 1-grams in blog corpus", cex.main=0.9)
par(mar=c(5.5, 4, 2, 0))
col <- brewer.pal(9, "YlOrRd")
col <- colorRampPalette(rev(col))
barplot(head(gram2, 20), xlab="", col=col(20), las=2, ylim=c(0, 180000),
        main="Counts of 2-grams in blog corpus", cex.main=0.9)
par(mar=c(8, 4, 2, 0))
col <- brewer.pal(9, "YlGnBu")
col <- colorRampPalette(rev(col))
barplot(head(gram3, 20), xlab="", col=col(20), las=2, ylim=c(0, 20000),
        main="Counts of 3-grams in blog corpus", cex.main=0.9)
par(mar=c(10, 4, 2, 0))
col <- brewer.pal(9, "Purples")
col <- colorRampPalette(rev(col))
barplot(head(gram4, 20), xlab="", col=col(20), las=2, ylim=c(0, 4000),
        main="Counts of 4-grams in blog corpus", cex.main=0.9)
```

Punctuation marks were not removed from this graph, and we can see that the most frequent 1-gram is a ".". The most frequent 3-gram is "\<s\> \<digit\> ." where the tag "\<s\>" is the start of the document. This probably the specific feature of the blog corpus, because blog posts often have numeration, time and/or date at the beginning. Another interesting feature of the blog corpus is that the most frequent 4-gram in it is the abbreviation "U . S .", so we can conclude that either the data contains a lot of documents posted by bloggers from the United States or US-related discussions were among the hot topics in blogs at the time when the data was collected.

### Twitter Corpus

Twitter corpus consists of fewer 1-grams and 2-grams than blog or news corpus. This possibly because blogs and news usually have richer content with more sophisticated language and different contexts, while twits are generally short and consists of more common words. The number of 3 and 4-gram is roughly the same for twitter, blog, and news corpora.

```{r load_twitter, cache=TRUE, echo=FALSE}
library(RColorBrewer)
load("gram1.twitter.counts")
load("gram2.twitter.counts")
load("gram3.twitter.counts")
load("gram4.twitter.counts")
size <- unlist(list(one.gram=length(gram1), two.gram=length(gram2),
                  three.gram=length(gram3), four.gram=length(gram4)))
res <- data.frame(n.gram.size=size, row.names=names(size))
res
```

There are also some differences in the most common words frequencies:

```{r twitter_frequent_words, cache=TRUE, fig.height=4, fig.width=6, echo=FALSE}
par(mfrow=c(1, 1))
idx <- names(gram1) %in% c(".", "!", "?", ",", "<s>")
gr1 <- gram1[!idx]
col <- brewer.pal(9, "Blues")
col <- colorRampPalette(rev(col))
par(mar=c(6, 5, 3, 0))
barplot(head(gr1, 30), xlab="", ylab="Counts", col=col(30), las=2, ylim=c(0, 1200000),
        main="Most frequent words in twitter corpus", cex.axis=0.6)
```

While the most frequent words are generally also "stop-words" as in the blogs and news data, twitter corpus contains many "bad-words".

The most frequent n-grams are also differ significantly from the blogs and news corpora:

```{r twitter_grams,  cache=TRUE, fig.height=5, fig.width=7, echo=FALSE}
par(mfrow=c(2, 2))
par(mar=c(5, 5, 2, 0))
col <- brewer.pal(9, "Blues")
col <- colorRampPalette(rev(col))
barplot(head(gram1, 20), xlab="", col=col(20), las=2, ylim=c(0, 2000000),
        main="Counts of 1-grams in twitter corpus", cex.main=0.9)
par(mar=c(5.5, 4, 2, 0))
col <- brewer.pal(9, "Oranges")
col <- colorRampPalette(rev(col))
barplot(head(gram2, 20), xlab="", col=col(20), las=2, ylim=c(0, 200000),
        main="Counts of 2-grams in twitter corpus", cex.main=0.9)
par(mar=c(8, 4, 2, 0))
col <- brewer.pal(9, "BuGn")
col <- colorRampPalette(rev(col))
barplot(head(gram3, 20), xlab="", col=col(20), las=2, ylim=c(0, 25000),
        main="Counts of 3-grams in twitter corpus", cex.main=0.9)
par(mar=c(10, 4, 2, 0))
col <- brewer.pal(9, "BuPu")
col <- colorRampPalette(rev(col))
barplot(head(gram4, 20), xlab="", col=col(20), las=2, ylim=c(0, 20000),
        main="Counts of 4-grams in twitter corpus", cex.main=0.9)
```

The most frequent 1 and 2-grams in the twitter corpus contain "\<s\>" tag that means start of the document. This is not surprising, because twits are usually short, and corpus from twitter will have more documents than a corpus of the same size with blogs or news. 

Another feature of the twitter data is that most frequent 3 and 4-grams generally contain word "thank". It is probably due to the fact that there are a lot of twits like "thanks for the follow", "thanks for the rt", etc. in the corpus.

### News Corpus

News corpus consists of roughly the same number of different n-grams and have approximately the same most frequent words distribution as corpus with blogs:

```{r load_news, cache=TRUE, echo=FALSE}
library(RColorBrewer)
load("gram1.news.counts")
load("gram2.news.counts")
load("gram3.news.counts")
load("gram4.news.counts")
size <- unlist(list(one.gram=length(gram1), two.gram=length(gram2),
                  three.gram=length(gram3), four.gram=length(gram4)))
res <- data.frame(n.gram.size=size, row.names=names(size))
res
```

```{r news_frequent_words, cache=TRUE, fig.height=4, fig.width=6, echo=FALSE}
par(mfrow=c(1, 1))
idx <- names(gram1) %in% c(".", "!", "?", ",", "<s>")
gr1 <- gram1[!idx]
col <- brewer.pal(9, "Oranges")
col <- colorRampPalette(rev(col))
par(mar=c(6, 5, 3, 0))
barplot(head(gr1, 30), xlab="", ylab="Counts", col=col(30), las=2, ylim=c(0, 2000000),
        main="Most frequent words in news corpus", cex.axis=0.6)
```

There are different the most common 3 and 4 grams in the news corpus comparing to the other. They have a lot of digits and time abbreviations like "a . m .", "p . m ." or their combinations. There is also "u . s ." abbreviation frequent there as in the blogs corpus:

```{r news_grams,  cache=TRUE, fig.height=5, fig.width=7, echo=FALSE}
par(mfrow=c(2, 2))
par(mar=c(5, 5, 2, 0))
col <- brewer.pal(9, "Oranges")
col <- colorRampPalette(rev(col))
barplot(head(gram1, 20), xlab="", col=col(20), las=2, ylim=c(0, 2000000),
        main="Counts of 1-grams in news corpus", cex.main=0.9)
par(mar=c(5.5, 4, 2, 0))
col <- brewer.pal(9, "Greens")
col <- colorRampPalette(rev(col))
barplot(head(gram2, 20), xlab="", col=col(20), las=2, ylim=c(0, 200000),
        main="Counts of 2-grams in news corpus", cex.main=0.9)
par(mar=c(8, 4, 2, 0))
col <- brewer.pal(9, "Reds")
col <- colorRampPalette(rev(col))
barplot(head(gram3, 20), xlab="", col=col(20), las=2, ylim=c(0, 30000),
        main="Counts of 3-grams in news corpus", cex.main=0.9)
par(mar=c(10, 4, 2, 0))
col <- brewer.pal(9, "PuBuGn")
col <- colorRampPalette(rev(col))
barplot(head(gram4, 20), xlab="", col=col(20), las=2, ylim=c(0, 20000),
        main="Counts of 4-grams in twitter corpus", cex.main=0.9)
```

## Predictive Model

### Probability model

The n-gram counts were transformed into probabilities to have the last word in a n-gram given the previous words. For each n-gram this probability might be calculated as a count of the particular n-gram divided on a count of (n-1)-gram (the same n-gram but without the last word).

```{r prob_model, echo=FALSE}
getProbability <- function(counts, ngram_minus_one) {
    # calculates conditional probability of having the last word in n-gram
    # given the rest of n-gram
    xgram <- integer(0)
    for (index in 1:length(counts)) {
        item <- counts[index]
        name <- names(item)
        splitted <- strsplit(name, split=" ")[[1]]
        name_minus_one <- splitted[1:length(splitted)-1]
        xgram <- c(xgram, item[[1]] / ngram_minus_one[[paste(name_minus_one, collapse=" ")]])
        if (index %% 50000 == 0){
#             print(paste(index, "elements was evaluated", date())) # performance testing
            save(xgram, file=paste0("temp.file_", index))
            xgram <- integer(0)
        }
    }
    save(xgram, file=paste0("temp.file_", index))
    rm(xgram)
    files_to_collapse <- grep("temp.file_", dir(), value=TRUE)
    files_to_collapse <- files_to_collapse[order(as.numeric(gsub("temp.file_", "", files_to_collapse)))]
    result <- integer(0)
    for (file in files_to_collapse) {
        load(file)
        result <- c(result, xgram)
        rm(xgram)
    }
    names(result) <- names(counts)
    file.remove(files_to_collapse)
    return(result)
}
```

```{r prob_count, cache=TRUE, echo=FALSE}
if (!status[length(status)]){
    load("gram1.twitter.counts")
    load("gram2.twitter.counts")
    bigram <- getProbability(gram2, gram1)
    rm(gram1)
    save(bigram, file="twitter.bigram")
    rm(bigram)
    load("gram3.twitter.counts")
    threegram <- getProbability(gram3, gram2)
    rm(gram2)
    save(threegram, file="twitter.threegram")
    rm(threegram)
    load("gram4.twitter.counts")
    fourgram <- getProbability(gram4, gram3)
    rm(gram3, gram4)
    save(fourgram, file="twitter.fourgram")
    load("twitter.bigram")
    load("twitter.threegram")
    model <- list(bigram=sort(bigram, decreasing=TRUE), 
                  threegram=sort(threegram, decreasing=TRUE), 
                  fourgram=sort(fourgram, decreasing=TRUE))
    save(model, file="twitter.model")
    rm(bigram, threegram, fourgram, model)
    
    
    load("gram1.blogs.counts")
    load("gram2.blogs.counts")
    bigram <- getProbability(gram2, gram1)
    rm(gram1)
    save(bigram, file="blogs.bigram")
    rm(bigram)
    load("gram3.blogs.counts")
    threegram <- getProbability(gram3, gram2)
    rm(gram2)
    save(threegram, file="blogs.threegram")
    rm(threegram)
    load("gram4.blogs.counts")
    fourgram <- getProbability(gram4, gram3)
    rm(gram3, gram4)
    save(fourgram, file="blogs.fourgram")
    load("blogs.bigram")
    load("blogs.threegram")
    model <- list(bigram=sort(bigram, decreasing=TRUE), 
                  threegram=sort(threegram, decreasing=TRUE), 
                  fourgram=sort(fourgram, decreasing=TRUE))
    save(model, file="blogs.model")
    rm(bigram, threegram, fourgram, model)
    
    
    load("gram1.news.counts")
    load("gram2.news.counts")
    bigram <- getProbability(gram2, gram1)
    rm(gram1)
    save(bigram, file="news.bigram")
    rm(bigram)
    load("gram3.news.counts")
    threegram <- getProbability(gram3, gram2)
    rm(gram2)
    save(threegram, file="news.threegram")
    rm(threegram)
    load("gram4.news.counts")
    fourgram <- getProbability(gram4, gram3)
    rm(gram3, gram4)
    save(fourgram, file="news.fourgram")
    load("news.bigram")
    load("news.threegram")
    
    model <- list(bigram=sort(bigram, decreasing=TRUE), 
                  threegram=sort(threegram, decreasing=TRUE), 
                  fourgram=sort(fourgram, decreasing=TRUE))
    save(model, file="news.model")
    rm(bigram, threegram, fourgram, model)
}    
```

After calculating probabilities for each n-gram, they were placed into the model objects (three objects, separately for twits, news, and blogs).

### Optimization

Optimization is heavily dependant on the requirements for the final product. So we should describe how the final Web application is expected to work. It have to contain text input form, where user may type the text. Data from this form should be processed with predictive function that returns most probable next words using our n-gram model. It also might return more than one word, but some reasonable number of words to give a possibility to choose the right one by user itself. The goal of this application is to reduce the amount of typing, because it might be not very convenient on mobile devices. So giving 5 most probable next words to a user should be a reasonable number of words to choose from. Five items could be easily showed at a mobile device screen.

Taking to consideration provided description of the final product, we can think how our predictive models might be optimized. We do not require more than 5 predictions for each input text, so we can reduce the size of models by deleting all predictions beside the top-5 for each n-gram (without removing the punctuation marks, they will be treated separately by the predictive function), because they will never appear in predictions. So at the end we should have predictive models with no more than 5 predictions for each n-gram, plus predictions of punctuation marks.

```{r optimization_func, echo=FALSE}

optFunc <- function(ngram){
    # optimizes n-gram model removing unnecessary for predictions elements 
    # that has low probability
    n <- length(strsplit(names(ngram[1]), split=" ")[[1]])
    ngram <- ngram[order(names(ngram))]
    names_minus_one <- sapply(names(ngram), 
                              function (x) paste(strsplit(x, split=" ")[[1]][1:n-1], 
                                                 collapse=" "))
    names_minus_one <- unname(names_minus_one)
    names_minus_one <- as.numeric(as.factor(names_minus_one))
    counts <- table(names_minus_one)
    rm(names_minus_one)
    index <- list()
    start <- 1
    for (x in 1:length(names(counts))) {
        num <- unname(counts[x])
        start_stop <- c(start, start + num - 1)
        index[x] <- list(start_stop)
        start <- start + num
        if (start %% 10000 == 0) print(paste("creating index", start, date())) 
    }
    print("----------------------------------------------------------")
    counts <- counts[counts > 5]
    print(length(counts))
    result <- numeric(0)
    end <- 1
    for (item in names(counts)) {
        start_end <- index[[as.numeric(item)]]
        result <- c(result, ngram[end:start_end[1] - 1])
        similar <- sort(ngram[start_end[1]:start_end[2]], decreasing=TRUE)
        last_words <- unlist(lapply(names(similar), 
                                    function(x) strsplit(x, split=" ")[[1]][n]))
        last_words <- !last_words %in% c(".", "!", ",", "?", "<digit>", "<e-mail>",
                                         "<badword>")
        similar <- head(similar[last_words], 5)
        result <- c(result, similar)
        end <- start_end[2] + 1
        if (end %% 100 == 0) print(paste("Optimizing", end, date()))
    }
    result <- c(result, ngram[end:length(ngram)])
    return(result)
}
```

```{r optimization, echo=FALSE}
if (!(file.exists("optimized.twitter.model") & 
        file.exists("optimized.blogs.model") &
        file.exists("optimized.news.model"))) {
    load("twitter.model")
    model <- lapply(model, optFunc)
    model_twitter <- model
    rm(model)
    save(model_twitter, file="optimized.twitter.model")
    rm(model_twitter)
    load("blogs.model")
    model <- lapply(model, optFunc)
    model_blogs <- model
    rm(model)
    save(model_blogs, file="optimized.blogs.model")
    rm(model_blogs)
    load("news.model")
    model <- lapply(model, optFunc)
    model_news <- model
    rm(model)
    save(model_news, file="optimized.news.model")
    rm(model_news)
}
```

Here you can see the size of models before and after the optimization:

```{r optimization_models, echo=FALSE, cache=TRUE}
if (!(file.exists("twit.optimization.results") & 
        file.exists("blog.optimization.results") &
        file.exists("news.optimization.results"))) {
    load("twitter.model")
    print("Twitter model length before and after the optimization:")
    before <- unlist(lapply(model, length))
    rm(model)
    load("optimized.twitter.model")
    after <- unlist(lapply(model_twitter, length))
    res <- data.frame(before=before, after=after)
    print(res)
    save(res, file="twit.optimization.results")
    rm(model_twitter, before, after, res)
    
    load("blogs.model")
    print("Blogs model length before and after the optimization:")
    before <- unlist(lapply(model, length))
    rm(model)
    load("optimized.blogs.model")
    after <- unlist(lapply(model_blogs, length))
    res <- data.frame(before=before, after=after)
    print(res)
    save(res, file="blog.optimization.results")
    rm(model_blogs, before, after, res)
    
    load("news.model")
    print("News model length before and after the optimization:")
    before <- unlist(lapply(model, length))
    rm(model)
    load("optimized.news.model")
    after <- unlist(lapply(model_news, length))
    res <- data.frame(before=before, after=after)
    print(res)
    save(res, file="news.optimization.results")
    rm(model_news, before, after, res)
} else {
    print("Twitter model length before and after the optimization:")
    load("news.optimization.results")
    print(res)
    rm(res)
    print("Blogs model length before and after the optimization:")
    load("blog.optimization.results")
    print(res)
    rm(res)
    print("News model length before and after the optimization:")
    load("news.optimization.results")
    print(res)
    rm(res)
}
```

Besides this, another optimization might be done to improve access to data. Next word prediction should be almost instantaneous in our App, because potential user will not be waiting for predictions for even small amount of time. Created n-gram models are still have many elements in them, and searching for the right one might take some time if we will try to access data as is. So we decided to make hashing of the models by grouping similar records using the first two characters in each n-gram, which makes access to data much more faster.

```{r indexing_func, echo=FALSE}
makeIndex <- function(ngram) {
    # takes n-gram vector and creates an index 
    # that is based on the first 2 characters of n-grams
    two_chars <- unname(sapply(names(ngram), substr, 1, 2))
    counts <- table(two_chars)
    index <- list()
    start <- 1
    for (name in names(counts)) {
        num <- unname(counts[name])
        start_stop <- c(start, start + num - 1)
        index[name] <- list(start_stop)
        start <- start + num
    }
    return(index)
}
```

```{r indexing, echo=FALSE}
if (!all(file.exists("blogs.model.hashed", "news.model.hashed", "twitter.model.hashed",
                 "news.unigram.hashed", "news.unigram.hashed", "news.unigram.hashed"))) {
    load("optimized.blogs.model")
    model_blogs <- lapply(model_blogs, function (x) x[order(names(x))])
    hash_blogs <- lapply(model_blogs, makeIndex)
    save(list=c("model_blogs", "hash_blogs"),  file="blogs.model.hashed")
    rm(hash_blogs, model_blogs)
    
    load("optimized.news.model")
    model_news <- lapply(model_news, function (x) x[order(names(x))])
    hash_news <- lapply(model_news, makeIndex)
    save(list=c("model_news", "hash_news"),  file="news.model.hashed")
    rm(hash_news, model_news)
    
    load("optimized.twitter.model")
    model_twitter <- lapply(model_twitter, function (x) x[order(names(x))])
    hash_twitter <- lapply(model_twitter, makeIndex)
    save(list=c("model_twitter", "hash_twitter"),  file="twitter.model.hashed")
    rm(hash_twitter, model_twitter)
    
    load("gram1.news.counts")
    gram1 <- gram1[gram1 >= 10]
    gram1_news <- gram1[order(names(gram1))]
    uni_hash_news <- makeIndex(gram1_news)
    save(list=c("gram1_news", "uni_hash_news"),  file="news.unigram.hashed")
    rm(uni_hash_news, gram1, gram1_news)
    load("gram1.blogs.counts")
    gram1 <- gram1[gram1 >= 10]
    gram1_blogs <- gram1[order(names(gram1))]
    uni_hash_blogs <- makeIndex(gram1_blogs)
    save(list=c("gram1_blogs", "uni_hash_blogs"),  file="blogs.unigram.hashed")
    rm(uni_hash_blogs, gram1_blogs, gram1)
    load("gram1.twitter.counts")
    gram1 <- gram1[gram1 >= 10]
    gram1_twitter <- gram1[order(names(gram1))]
    uni_hash_twitter <- makeIndex(gram1_twitter)
    save(list=c("gram1_twitter", "uni_hash_twitter"),  file="twitter.unigram.hashed")
    rm(uni_hash_twitter, gram1_twitter, gram1)
}
```

### Predictive Function

The predictive function uses n-gram models and indexes to predict the most probable next words. It produces a prediction that consists of 5 words at most.

When argument clean=TRUE, it also perform text preprocessing before generating predictions (similar to the corpus cleaning during the model-creation process). At the cleaning stage all digits, e-mails and bad-words are replaced with coresponding tokens, and special characters are removed (except the meaningful punctuation marks ".,?."), as well as excessive white-space characters. At the beginning of the text the special tag "\<s\>" is placed.

The predicting part of this function uses "backing-off" to the lower n models under conditions when the given n-gram model do not produce a good prediction. By doing this, it produce the most reliable prediction using the best performing model. The backing-of in the function is from 4 to 3 and 2-gram models.

Smoothing wasn't used for the models, because during the tests it produced less accurate predictions than just using simple "backing-off" without smoothing.

Another feature of the particular predictive function is that it's behaviour depends on the last character in the text. When there is a "space" at this position, it includes punctuation mark into results, so it also predicts those four important punctuations - ".,!?".

```{r pred_func, echo=FALSE}
predictNextWord <- function(char, model, hash, unigram, uni_hash, clean=TRUE) {
    # predicts next 5 most probable words for the character string 
    
    # status check if the "space" after the last word in char string was typed
    initial <- char
    status <- substr(char, nchar(char), nchar(char)) == " "
    
    # cleaning char
    if (clean) {
        char <- gsub("[<>]", "", char)
        char <- gsub("[[:digit:]]+[:.,-][[:digit:]]+", " <digit> ", char)
        char <- gsub("[[:digit:]]+", " <digit> ", char)
        char <- gsub("[[:alnum:].-]+@[[:alnum:].-]+", " <e-mail> ", char)
        for (smile in c("x-p", "o.o", "[=:;][boqje#pds]")) {
            char <- gsub(smile, "", char)
            }
        char <- gsub("\\?+\\!+|\\!+\\?+", "?", char)
        char <- gsub(";+", ",", char)
        char <- gsub('[])(:#%$^\\~{}[&+=@/"_]+', "", char)
        for (punct in c("!", "?", ".", ",")) {
            pattern <- paste0("\\", punct, "+")
            replacement <- paste0(" ", punct, " ")
            char <- gsub(pattern, replacement, char)
            char <- gsub("\\s+", " ", char)
            }
        reg <- " screw | wank[a-z]*|b[o0][o0]bs|l3i[a-z]ch|knob|or[ia]f[ai][cs]|[a-z]damn |d[iy]ck|q[uw]e[ei]r| arse|[ck]awk| gay|retard|s[kc]anc*k|t[ie]+t[sz]|slut|puss[ye]|vul+va|vag*[i1]*j*i*na|pola[ck]|phu[ck]|pe[ie]*n[a1iu]+s|pac*k[iy]| anal | anus |butt[wh-]|clit|crap|pu[tl][eao] |jar*c*k[\\s-]*off|blow\\s*job|ji[zs]+|nazi|w*h[o0]+a*r|a[s]*h[o0]le|sh[i1!y]t|mas+t[eu]*rbai*t|n[i1]+g+[eu]*r| a[sz][sz]|bas+t[ae]rd|b[!i1][a7]*tch|c[o0]ck| cum |[ck]u*nt|dild[a-z]*|f\\s*u\\s*c*\\s*k|fai*g"
        char <- gsub(reg, " <badword> ", char)
        char <- gsub("^\\s+|\\s+$", "", char)
        char <- gsub("\\s+", " ", char)
        char <- paste("<s>", char)
    }
    last_three_words <- tail(strsplit(tolower(char), split=" ")[[1]], 3)
    first_two_chars_last_word <- substr(tail(last_three_words, 1), 1, 2)
    
    # getting top predictions
    res <- character(0)
    to_prediction <- paste(last_three_words, collapse=" ")
    if (length(last_three_words) == 3) {
        # try 4-gram
        two_chars <- substr(to_prediction, 1, 2)
        start_stop <- hash$fourgram[[two_chars]]
        if (!is.null(start_stop)) {
            indexes <- grep(paste0("^", to_prediction, " "), 
                            names(model$fourgram[start_stop[1]:start_stop[2]]))
            if (length(indexes) != 0) {
                res <- c(res, head(sort(model$fourgram[start_stop[1]:start_stop[2]][indexes],
                                        decreasing=TRUE), 15))
            }
        }
    }
    if (length(last_three_words) >= 2) {
        # try 3-gram
        to_prediction <- paste(tail(last_three_words, 2), collapse=" ")
        two_chars <- substr(to_prediction, 1, 2)
        start_stop <- hash$threegram[[two_chars]]
        if (!is.null(start_stop)) {
            indexes <- grep(paste0("^", paste(to_prediction, collapse=" "), " "), 
                            names(model$threegram[start_stop[1]:start_stop[2]]))
            if (length(indexes) != 0) {
                res <- c(res, head(sort(model$threegram[start_stop[1]:start_stop[2]][indexes],
                                        decreasing=TRUE), 15))
            }
        }
    }
    # try 2-gram
    to_prediction <- paste(tail(last_three_words, 1), collapse=" ")
    two_chars <- substr(to_prediction, 1, 2)
    if (nchar(two_chars) != 2) {
        two_chars <- paste0(two_chars, " ")
    }
    start_stop <- hash$bigram[[two_chars]]
    if (!is.null(start_stop)) {
        indexes <- grep(paste0("^", to_prediction, " "), 
                        names(model$bigram[start_stop[1]:start_stop[2]]))
        if (length(indexes) != 0) {
            res <- c(res, head(sort(model$bigram[start_stop[1]:start_stop[2]][indexes], 
                                    decreasing=TRUE), 15)) 
        }
    }
    
    # processing output
    res <- names(res)
    res <- sapply(res, function(x) tail(strsplit(x, split=" ")[[1]], 1))
    res <- res[!res %in% c("<digit>", "<badword>", "<e-mail>")]
    if (status) {
        res <- res[!res %in% c("?", "!", ",", ".")]
    } else {
        if (nchar(initial) > 0) {
            res[!res %in% c("?", "!", ",", ".")] <- paste0(" ", res[!res %in% c("?", 
                                                                                "!", 
                                                                                ",", 
                                                                                ".")])
        }
#         start_stop <- uni_hash[[first_two_chars_last_word]] # word completing, for App only
#         if (!tail(last_three_words, 1) %in% names(unigram[start_stop[1]:start_stop[2]])) {
#             print(tail(last_three_words, 1)) # for the application only
#             ind <- grep(paste0("^", tail(last_three_words, 1)),
#                         names(unigram[start_stop[1]:start_stop[2]]))
#             res <- unigram[start_stop[1]:start_stop[2]][ind]
#             res <- head(names(sort(res, decreasing=TRUE)), 5)
#             res <- gsub(tail(last_three_words, 1), "", res)
#         }
    }
    res <- unique(unname(head(res, 5)))
    ind <- logical(0)
    for (item in res) {
        word_chars <- strsplit(item, split="")
        ind <- c(ind, all(sapply(word_chars, 
                                 function(x) x %in% c(letters, "?", "!", ",", ".", " "))))
    }
    return(res[ind])
}

```

Lets try to predict the next words for the 4-grams from the phrase:

"The alacrity of the brown fox contributes to its ability to perform a saltatory action over the lazy dog"


```{r pred, echo=FALSE}
load("blogs.model.hashed")
load("blogs.unigram.hashed")
phrase <- "The alacrity of the brown fox contributes to its ability to perform a saltatory action over the lazy dog"
four_grams <- character(0)
tokens <- strsplit(phrase, split=" ")[[1]]
for (index in 1:(length(tokens)-3)) {
    four_grams <- c(four_grams, paste(tokens[index:(index+3)], collapse=" "))
}
for (gram4 in four_grams) {
    print(gram4)
    print(predictNextWord(gram4, model_blogs, hash_blogs, gram1_blogs, uni_hash_blogs))
    print(system.time(predictNextWord(gram4, model_blogs, hash_blogs, gram1_blogs, uni_hash_blogs)))
    print("----------------------------------------------------")
}
```

As we can see, the predictive function works, but to evaluate the accuracy of it large scale accuracy tests should be performed. They will be described in more details in the next segment of this report.

### Performance Testing

To calculate how well the models perform, we used the parts of files that was considered as the test sets, and which have not been used for the models creation:

1. en_US.twitter.txt - last 360,148 lines at the end of the file
2. en_US.blogs.txt - last 149,288 lines at the end of the file
3. en_US.news.txt - last 210,238 lines at the end of the file

To obtain the accuracy of the word prediction algorithm the following steps was performed:

1. 4-grams and their counts were generated from test sets.
2. There were randomly sampled 1000 4-grams from each set for testing.
3. Each 4-gram was separated on the first 3 words (which were used for prediction) and a last word (to compare result with).
4. The prediction of at most 5 words was generated using the predicting function, and the real last word from 4-gram was searched in vector with predictions. This produced True or False (1 or 0) prediction for each 4-gram.
5. The results (0 or 1) for each particular 4-gram was multiplied by the counts for it. After doing this for all 4-grams, the counts for all 4-gram were summed and compared to their sum before running these tests. We did this because not every 4-gram have the same weight for our ability to predict, and their counts represents frequencies of them in the real data. So their counts were used as weights. For instance, 4-gram with count 100 will happen 100 times more frequently than 4-gram with count 1, and it also will be 100 times more valuable for prediction.

Here are the results of prediction accuracy for all three models (models were tested in context of the data - twitter test set was used for the twitter model testing, etc.):

```{r gram4_testing, echo=FALSE}
if (!all(file.exists("testing.twitter", "testing.blogs", "testing.news"))) {
    files <- grep("ngram_", dir(), value=TRUE)
    file.remove(files)
    
    parseData("en_US.news.txt", chunksize=100, start_from=8001, end_at=100000)
    separateNgramFiles(grep("ngram_", dir(), value=TRUE))
    files <- grep("ngram_[[:digit:]]+_[123]", dir(), value=TRUE)
    file.remove(files)
    mergeAll(4)
    files <- grep("ngram_", dir(), value=TRUE)
    load(files)
    testing <- gram4
    rm(gram4)
    save(testing, file="testing.news")
    file.remove(files)
    
    parseData("en_US.twitter.txt", chunksize=500, start_from=4001, end_at=4000)
    separateNgramFiles(grep("ngram_", dir(), value=TRUE))
    files <- grep("ngram_[[:digit:]]+_[123]", dir(), value=TRUE)
    file.remove(files)
    mergeAll(4)
    files <- grep("ngram_", dir(), value=TRUE)
    load(files)
    testing <- gram4
    rm(gram4)
    save(testing, file="testing.twitter")
    file.remove(files)
    
    parseData("en_US.blogs.txt", chunksize=50, start_from=15001, end_at=15000)
    separateNgramFiles(grep("ngram_", dir(), value=TRUE))
    files <- grep("ngram_[[:digit:]]+_[123]", dir(), value=TRUE)
    file.remove(files)
    mergeAll(4)
    files <- grep("ngram_", dir(), value=TRUE)
    load(files)
    testing <- gram4
    rm(gram4)
    save(testing, file="testing.blogs")
    file.remove(files)
}
```

```{r testing, echo=FALSE}

if (!all(file.exists("testing.twitter", "testing.news", "testing.blogs"))) {
    load("testing.news")
    set.seed(777)
    testing <- sample(testing, 1000)
    load("news.model.hashed")
    load("news.unigram.hashed")
    testing <- sort(testing, decreasing=TRUE)
    before <- sum(testing)
    count <- 0
    for (index in 1:length(testing)) {
        item <- testing[index]
        name <- strsplit(names(item), split=" ")[[1]]
        last_word <- tail(name, 1)
        name <- paste0(paste(name[1:length(name)-1], collapse=" "), " ")
        predictions <- predictNextWord(name, model_news, hash_news, gram1_news, 
                                       uni_hash_news, clean=FALSE)
        result <- as.numeric(last_word %in% predictions)
        item <- item * result
        testing[index] <- item
        count <- count + 1
        if (count %% 10 == 0) print(paste(count, "n-grams evaluated", date()))
        }
    after <- sum(testing)
    accuracy <- after / before
    print("Word prediction accuracy for news n-gram model:")
    accuracy
    save(accuracy, file="news.accuracy")
    
    
    load("testing.blogs")
    testing <- sample(testing, 1000)
    load("blogs.model.hashed")
    load("blogs.unigram.hashed")
    testing <- sort(testing, decreasing=TRUE)
    before <- sum(testing)
    count <- 0
    for (index in 571:length(testing)) {
        item <- testing[index]
        name <- strsplit(names(item), split=" ")[[1]]
        last_word <- tail(name, 1)
        name <- paste0(paste(name[1:length(name)-1], collapse=" "), " ")
        predictions <- predictNextWord(name, model_blogs, hash_blogs, gram1_blogs, 
                                       uni_hash_blogs, clean=FALSE)
        result <- as.numeric(last_word %in% predictions)
        item <- item * result
        testing[index] <- item
        count <- count + 1
        if (count %% 10 == 0) print(paste(count, "n-grams evaluated", date()))
        }
    after <- sum(testing)
    accuracy <- after / before
    print("Word prediction accuracy for blogs n-gram model:")
    accuracy
    save(accuracy, file="blogs.accuracy")

    
    load("testing.twitter")
    testing <- sample(testing, 1000)
    load("twitter.model.hashed")
    load("twitter.unigram.hashed")
    testing <- sort(testing, decreasing=TRUE)
    before <- sum(testing)
    count <- 0
    for (index in 1:length(testing)) {
        item <- testing[index]
        name <- strsplit(names(item), split=" ")[[1]]
        last_word <- tail(name, 1)
        name <- paste0(paste(name[1:length(name)-1], collapse=" "), " ")
        predictions <- predictNextWord(name, model_twitter, hash_twitter, 
                                       gram1_twitter, uni_hash_twitter, clean=FALSE)
        result <- as.numeric(last_word %in% predictions)
        item <- item * result
        testing[index] <- item
        count <- count + 1
        if (count %% 10 == 0) print(paste(count, "n-grams evaluated", date()))
        }
    after <- sum(testing)
    accuracy <- after / before
    print("Word prediction accuracy for twitter n-gram model:")
    accuracy
    save(accuracy, file="twitter.accuracy")

    
} else {
    load("news.accuracy")
    print("Word prediction accuracy for news n-gram model:")
    print(accuracy)
    
    load("blogs.accuracy")
    print("Word prediction accuracy for blogs n-gram model:")
    print(accuracy)
    
    load("twitter.accuracy")
    print("Word prediction accuracy for twitter n-gram model:")
    print(accuracy)
}
```

## Web Application

Based on the obtained predictive function and optimized models, the demonstrational Web application was created. Here are some features of it's functionality:

+ It predicts the most probable next words using back-off n-gram model
+ Prediction consists of 5 words or less to choose the right one by user itself
+ The are possibility to choose one of three different modes for prediction (twitter, news, blog) for different kinds of texts 
+ When user types just a part of word the application is trying to predict the rest of the word
+ While the application does not predict profane words, bad-words can be in the user's text and they do not affect the quality of prediction
+ App includes punctuation marks into prediction results when the last character in the user's text is not a "space" character
+ The predictions are not affected by the proximity of the document start. App will try to predict the next word even if the document is empty
+ It can predict words that follow after e-mails or digits. Also it predicts abbreviations like U.S., p.m. and others.

The application can be accessed here:
[Under Development]()
